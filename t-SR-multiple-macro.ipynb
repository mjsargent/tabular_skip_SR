{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76305468",
   "metadata": {},
   "source": [
    "## Temporally Extended Successor Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61ff32",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ac04b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mjsargent/anaconda3/envs/JAX/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/mjsargent/Repos/tabular_skip_SR/utils.py:27: MatplotlibDeprecationWarning: You are modifying the state of a globally registered colormap. This has been deprecated since 3.3 and in 3.6, you will not be able to modify a registered colormap in-place. To remove this warning, you can make a copy of the colormap first. cmap = mpl.cm.get_cmap(\"cividis\").copy()\n",
      "  current_cmap.set_bad(color='grey')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import utils \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c73ca",
   "metadata": {},
   "source": [
    "### (Hyper)parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdbd8fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "MAX_SKIP = 7\n",
    "EPS = 0.01\n",
    "ALPHA = 0.9\n",
    "N_MACRO = 3\n",
    "ENV_NAME = \"four_way_junction\" # four_rooms, junction_hard, open_field_10, open_field 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "155b85c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ts, env = utils.make_transition_functions(ENV_NAME)\n",
    "dims = utils.get_dims(env)\n",
    "utils.plot_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f96be66",
   "metadata": {},
   "source": [
    "### Vanilla successor representation\n",
    "$$\\mathbf{M}^{\\pi}(s, s') = (I - \\gamma T^{\\pi})^{-1}$$\n",
    "$$\\pi = \\text{uniform random} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b533d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16.14688601 12.7588814   9.88638716  7.4133429   3.66679964  3.81495316\n",
      "  4.1172462   4.58589263  5.23982764  4.58589263  4.1172462   3.81495316\n",
      "  3.66679964  4.58589263  4.1172462   3.81495316  3.66679964]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M_uniform = utils.compute_random_walk_SR(Ts, gamma = GAMMA)\n",
    "print(M_uniform[0, :])\n",
    "utils.plot_SR(M_uniform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5fb4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAADkElEQVR4nO3cMWpUURSA4TdhgiRWopWt2AhpTGdjq41LyFYmsxWX4C4kaClpbK0EO4MoeW5gZoTg9f3I95XvwuE2/xyY4q7meZ6AnqOlLwDsJk6IEidEiROixAlR60OH2+3WX7kLu3z7Yez8i/Oh8/mzzWaz2vXd5oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROiDr5bS8DR8dI3YCE2J0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDl3dq69enSN2AhNidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6I8qh03b0HS9+AhdicECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtRqnuf9h0/e7D/8G46Oh46f1qdj50/T8HdlH5+M/f38cnM7dP7049vY+dM0Tb++j51/+3Po+Pnzu9Wu7zYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcELU+dHh5cf6v7sEeHz99HTr/+bNHQ+dP08PB8/9fNidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQdfDdWpZ39vT+0ldgITYnRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiPKodNyLs5uh86+uT4bO5+5sTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCHKu7Vxr16+Hzr/6vr10Pncnc0JUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRK3meV76DsAONidEiROixAlR4oQocUKUOCHqN2jUKceix+cwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.plot_SR_column(M_uniform, env, s= 16, dims = dims, title = \"\", show = True, save = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e86ec",
   "metadata": {},
   "source": [
    "### Online Macro Action Discovery - Multiple Macro Actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867ea24f",
   "metadata": {},
   "source": [
    "Here we will try macro action discovery over a set of potential macro actions. We will try this in the simplest way possible: using a precomputed baseline SR; fixing the starting state, and randomly sampling the macro actions. We will compute their tSRs, and use the difference between the baseline and tSR as a reward in our abstract MDP.  \n",
    "\n",
    "In this highly simplifed setting, we hope to recover three macro actions: one for each end of three reachable corridors. We will help even more by setting the length of our macro actions to match the length of the desired trajectories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c690b22",
   "metadata": {},
   "source": [
    "#### Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a457f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_abstract_reward(M, M_baseline, M_set, Q_set, mac_idx,states, actions, beta = 0.5, diverse_intrinsic = False):\n",
    "    \n",
    "    abstract_reward = [np.sum(M[(states[0])][_s] - M_baseline[states[0], _s])  for _s, a in zip(states[1:], actions)]\n",
    "    if diverse_intrinsic:\n",
    "        # list the idx of the macros we are not following\n",
    "        intrinsic_reward = 0\n",
    "        \n",
    "        other_idxs = np.arange(len(M_set.keys()))\n",
    "        other_idxs = np.delete(other_idxs, [mac_idx], 0)\n",
    "        \n",
    "        for other_idx in other_idxs:\n",
    "            \n",
    "            q = Q_set[other_idx]\n",
    "            for t in range(len(q.values())):\n",
    "                a_other = np.random.choice(np.where(q[t] == q[t].max())[0])\n",
    "                intrinsic_reward += abs(int(a_other == actions[t]) - 1)\n",
    "                \n",
    "            # find the sequence of actions implied by this macro action\n",
    "        abstract_reward = [ab + beta*intrinsic_reward for ab in abstract_reward]\n",
    "    return abstract_reward \n",
    "            \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "038472ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on policy SARSA style TD update rules for random walk\n",
    "\n",
    "def train_random_walk_multiple_macro(n_traj = 100000, macro_length = 8, n_macro = 3, beta = 0.5, diverse_intrinsic = False, start_state = 16):\n",
    "    # fixed starting state\n",
    "    s = start_state\n",
    "    \n",
    "    action_map = {0: \"UP\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"LEFT\"}\n",
    "    \n",
    "    # define abstract Q values and SRs for macro actions\n",
    "    macro_action_Q = {i: {j: np.zeros(len(action_map.keys())) for j in range(macro_length)} \\\n",
    "                       for i in range(n_macro)} \n",
    "    macro_action_M = {i: defaultdict(lambda: np.zeros(dims[\"states\"])) for i in range(n_macro)}\n",
    "                                                                   \n",
    "    # pre compute a baseline SR\n",
    "    M_baseline =  utils.compute_random_walk_SR(Ts, gamma = GAMMA)\n",
    "\n",
    "    # action space is only the macro actions\n",
    "                    \n",
    "    for i in range(n_traj):\n",
    "                                                                   \n",
    "        mac_idx = np.random.randint(n_macro)\n",
    "        macro_reward = np.zeros(dims[\"states\"])\n",
    "        # store rewards along the subtrajectory to update at the end so we know what actions we are taking\n",
    "        abstract_rewards = np.zeros(macro_length)\n",
    "        macro_rewards = np.zeros([macro_length, dims[\"states\"]])\n",
    "\n",
    "        states_seen = [s]\n",
    "        actions_taken = []\n",
    "        curr_Q_abst = macro_action_Q[mac_idx]\n",
    "        curr_M_macro = macro_action_M[mac_idx]\n",
    "        \n",
    "        for t in range(macro_length):\n",
    "            q_t = curr_Q_abst[t]\n",
    "            # awkward syntax for tie breaks\n",
    "            a = np.random.choice(np.where(q_t == q_t.max())[0])\n",
    "            # eps greedy - probably don't need it?\n",
    "            if np.random.rand(1) < EPS:\n",
    "                a = np.random.randint(4)\n",
    "            s_vector = Ts[action_map[a]][s]\n",
    "            s_ = int(np.where(s_vector == 1)[0])\n",
    "            states_seen.append(s_)\n",
    "            actions_taken.append(a)\n",
    "            s = s_\n",
    "        # now we have the sequence of actions and states, we can compute the abstract rewards\n",
    "\n",
    "           \n",
    "        abstract_reward = compute_abstract_reward(curr_M_macro, M_baseline, macro_action_M, macro_action_Q, mac_idx, states_seen, actions_taken, diverse_intrinsic = diverse_intrinsic)\n",
    "   \n",
    "        a_ = np.random.randint(4)\n",
    "        t = len(actions_taken) - 1\n",
    "        # final abstract state is always a terminal state - has zero value\n",
    "        q_tp1 = np.zeros(len(action_map.keys()))\n",
    "        # TODO seeing as we are doing this reverse loop, we could compute n-step returns\n",
    "        macro_reward = np.zeros(dims['states'])\n",
    "        \n",
    "        for a in reversed(actions_taken):\n",
    "            #s_vector = np.eye(dims[\"states\"])[s_]\n",
    "            #macro_reward += s_vector + GAMMA * macro_reward\n",
    "            #M_macro[(s, a)] += ALPHA * (sr_target - M_macro[(s,a)])\n",
    "\n",
    "            q_t = curr_Q_abst[t]\n",
    "            target = abstract_reward[t] + GAMMA * q_tp1[a_]\n",
    "            q_t[a] += ALPHA * (target - q_t[a])\n",
    "\n",
    "            a_ = a\n",
    "            q_tp1 = q_t\n",
    "            t+=-1\n",
    "            \n",
    "        #print(\"start loop\")\n",
    "        for t, s_ in enumerate(states_seen[1:]):\n",
    "            s_vector = np.eye(dims[\"states\"])[s_]\n",
    "            macro_reward += (GAMMA**t)*s_vector\n",
    "            \n",
    "        a_ = np.random.randint(4)\n",
    "        #print(macro_reward)\n",
    "        sr_target = macro_reward + (GAMMA**(len(actions_taken)))*curr_M_macro[(states_seen[-1])]\n",
    "        curr_M_macro[(states_seen[0])] += ALPHA * (sr_target - curr_M_macro[(states_seen[0])])\n",
    "        # make sure last action matches for next step\n",
    "        a = actions_taken[-1]\n",
    "        \n",
    "        # reset to starting state\n",
    "        s = start_state\n",
    "                                                                   \n",
    "                                                                   \n",
    "    return macro_action_Q, macro_action_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d25eb057",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_action_Q, macro_action_M,  = train_random_walk_multiple_macro(n_macro = N_MACRO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61dd79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ ← \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ → \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ← ← \n"
     ]
    }
   ],
   "source": [
    "for q in macro_action_Q.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1448d4e7",
   "metadata": {},
   "source": [
    "#### Macro Actions: Diversity with beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2aaa99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro(n_macro = N_MACRO, beta = 0.5, diverse_intrinsic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "760f5a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "← → ↑ ↑ ← ↑ ↑ ↑ \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ← ← ← → ↑ \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c4269",
   "metadata": {},
   "source": [
    "#### Macro Actions: Diversity with beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fcaeac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro(n_traj = 500000, n_macro = N_MACRO, beta = 1, diverse_intrinsic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6aa7e33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ → ← → ↑ ↑ ← \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ← ← ↑ → \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0decc0",
   "metadata": {},
   "source": [
    "#### Macro Actions: Diversity with beta = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7f27693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro(n_traj = 500000, n_macro = N_MACRO, beta = 2, diverse_intrinsic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b8de145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ ↑ → ↑ ↑ ↑ ← \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ → → ↑ ↑ \n",
      "Action Sequences\n",
      "↑ ↑ → ↑ ↑ ↑ → → \n"
     ]
    }
   ],
   "source": [
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e835a0b",
   "metadata": {},
   "source": [
    "#### Macro Actions: Diversity with beta = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f114b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro(n_traj = 500000, n_macro = N_MACRO, beta = 10, diverse_intrinsic = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b957f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ → ↑ \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ← ↑ → \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ → ↑ ↑ ↓ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ff6b2",
   "metadata": {},
   "source": [
    "These macro actions learn to traverse the first part of the corridor, (first four actions) but are not generally coherent. What if instead we use MC to update the abstract Q values instead of TD?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "197c36f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_walk_multiple_macro_abstract_MC(n_traj = 100000, macro_length = 8, n_macro = 3, beta = 0.5, diverse_intrinsic = False, start_state = 16):\n",
    "    # fixed starting state\n",
    "    s = start_state\n",
    "    \n",
    "    action_map = {0: \"UP\", 1: \"RIGHT\", 2: \"DOWN\", 3: \"LEFT\"}\n",
    "    \n",
    "    # define abstract Q values and SRs for macro actions\n",
    "    macro_action_Q = {i: {j: np.zeros(len(action_map.keys())) for j in range(macro_length)} \\\n",
    "                       for i in range(n_macro)} \n",
    "    macro_action_M = {i: defaultdict(lambda: np.zeros(dims[\"states\"])) for i in range(n_macro)}\n",
    "                                                                   \n",
    "    # pre compute a baseline SR\n",
    "    M_baseline =  utils.compute_random_walk_SR(Ts, gamma = GAMMA)\n",
    "\n",
    "    # action space is only the macro actions\n",
    "                    \n",
    "    for i in range(n_traj):\n",
    "                                                                   \n",
    "        mac_idx = np.random.randint(n_macro)\n",
    "        macro_reward = np.zeros(dims[\"states\"])\n",
    "        # store rewards along the subtrajectory to update at the end so we know what actions we are taking\n",
    "        abstract_rewards = np.zeros(macro_length)\n",
    "        macro_rewards = np.zeros([macro_length, dims[\"states\"]])\n",
    "\n",
    "        states_seen = [s]\n",
    "        actions_taken = []\n",
    "        curr_Q_abst = macro_action_Q[mac_idx]\n",
    "        curr_M_macro = macro_action_M[mac_idx]\n",
    "        \n",
    "        for t in range(macro_length):\n",
    "            q_t = curr_Q_abst[t]\n",
    "            # awkward syntax for tie breaks\n",
    "            a = np.random.choice(np.where(q_t == q_t.max())[0])\n",
    "            # eps greedy - probably don't need it?\n",
    "            if np.random.rand(1) < EPS:\n",
    "                a = np.random.randint(4)\n",
    "            s_vector = Ts[action_map[a]][s]\n",
    "            s_ = int(np.where(s_vector == 1)[0])\n",
    "            states_seen.append(s_)\n",
    "            actions_taken.append(a)\n",
    "            s = s_\n",
    "        # now we have the sequence of actions and states, we can compute the abstract rewards\n",
    "\n",
    "           \n",
    "        abstract_reward = compute_abstract_reward(curr_M_macro, M_baseline, macro_action_M, macro_action_Q, mac_idx, states_seen, actions_taken, diverse_intrinsic = diverse_intrinsic)\n",
    "   \n",
    "        a_ = np.random.randint(4)\n",
    "        t = len(actions_taken) - 1\n",
    "        # final abstract state is always a terminal state - has zero value\n",
    "        q_tp1 = np.zeros(len(action_map.keys()))\n",
    "        # TODO seeing as we are doing this reverse loop, we could compute n-step returns\n",
    "        macro_reward = np.zeros(dims['states'])\n",
    "        \n",
    "        abstract_acc = 0\n",
    "        for a in reversed(actions_taken):\n",
    "            abstract_acc = abstract_reward[t] + GAMMA * abstract_acc\n",
    "            q_t = curr_Q_abst[t]\n",
    "            \n",
    "            q_t[a] += ALPHA * (abstract_acc - q_t[a])\n",
    "            \n",
    "\n",
    "            a_ = a\n",
    "            q_tp1 = q_t\n",
    "            t+=-1\n",
    "            \n",
    "        #print(\"start loop\")\n",
    "        for t, s_ in enumerate(states_seen[1:]):\n",
    "            s_vector = np.eye(dims[\"states\"])[s_]\n",
    "            macro_reward += (GAMMA**t)*s_vector\n",
    "            \n",
    "        a_ = np.random.randint(4)\n",
    "        #print(macro_reward)\n",
    "        sr_target = macro_reward + (GAMMA**(len(actions_taken)))*curr_M_macro[(states_seen[-1])]\n",
    "        curr_M_macro[(states_seen[0])] += ALPHA * (sr_target - curr_M_macro[(states_seen[0])])\n",
    "        # make sure last action matches for next step\n",
    "        a = actions_taken[-1]\n",
    "        \n",
    "        # reset to starting state\n",
    "        s = start_state\n",
    "                                                                   \n",
    "                                                                   \n",
    "    return macro_action_Q, macro_action_M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf90c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro_abstract_MC(n_traj = 500000, n_macro = N_MACRO, beta = 1, diverse_intrinsic = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d41c361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ → \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ \n",
      "Action Sequences\n",
      "↑ ↑ ↑ ↑ → ↑ ← ← \n"
     ]
    }
   ],
   "source": [
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d95c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "macro_action_Q_diverse, macro_action_M_diverse = train_random_walk_multiple_macro_abstract_MC(n_traj = 500000, n_macro = N_MACRO, beta = 3, diverse_intrinsic = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "32214fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Sequences\n",
      "↑ ↑ → ↑ ↑ ↑ → ← \n",
      "Action Sequences\n",
      "→ ← → ↓ → → ↓ → \n",
      "Action Sequences\n",
      "← ↓ ↑ ← ↓ ↓ ↑ ↑ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for q in macro_action_Q_diverse.values():\n",
    "    utils.print_macro_actions(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08aefb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
